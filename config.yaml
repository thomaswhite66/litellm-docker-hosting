model_list:
  - model_name: ollama-llama3.1
    litellm_params:
      model: ollama/llama3.1:8b
      api_key: 'your-openai-api-key'
      api_base: 'http://host.docker.internal:11434'

# general_settings:
  # master_key: sk-1234
  # alerting: ['slack']
  # proxy_batch_write_at: 60
  # database_connection_pool_limit: 10 # limit the number of database connections to = MAX Number of DB Connections/Number of instances of litellm proxy (Around 10-20 is good number)
 
litellm_settings:
  request_timeout: 600 
  # set_verbose: False # Switch off Debug Logging, ensure your logs do not have any debugging on
  # json_logs: true # Get debug logs in json format
